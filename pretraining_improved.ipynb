{"cells":[{"cell_type":"markdown","metadata":{"id":"Cif91bTW-iBm"},"source":["# Initialization"]},{"cell_type":"markdown","metadata":{"id":"gqMTKe_bo3OW"},"source":["## Mount to drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1942,"status":"ok","timestamp":1684067616186,"user":{"displayName":"Alex Frasie","userId":"15148126022481281502"},"user_tz":-120},"id":"XolubFgCogTd","outputId":"8e7b3119-cff5-4eb1-e7e8-e1a4163888ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1684067616186,"user":{"displayName":"Alex Frasie","userId":"15148126022481281502"},"user_tz":-120},"id":"vh9SKjr9o7so","outputId":"72132c60-db92-42d7-91ef-5d666d446c2f"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/VUB/Thesis\n"]}],"source":["%cd /content/drive/My\\ Drive/VUB/Thesis"]},{"cell_type":"markdown","metadata":{"id":"g4yTdtjXS7r2"},"source":["## Install dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zW0fSA48S9se","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684067620514,"user_tz":-120,"elapsed":4331,"user":{"displayName":"Alex Frasie","userId":"15148126022481281502"}},"outputId":"04869be9-cd9a-4115-e181-7e2a2ed2a99d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.0.0+cu118)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (2.12.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (3.7.1)\n","Requirement already satisfied: openexr in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.3.9)\n","Requirement already satisfied: open3d in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (0.17.0)\n","Requirement already satisfied: kornia in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (0.6.12)\n","Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (0.11.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->-r requirements.txt (line 1)) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->-r requirements.txt (line 1)) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->-r requirements.txt (line 1)) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->-r requirements.txt (line 1)) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->-r requirements.txt (line 1)) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->-r requirements.txt (line 1)) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8->-r requirements.txt (line 1)) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8->-r requirements.txt (line 1)) (16.0.3)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 2)) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 2)) (1.54.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 2)) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 2)) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 2)) (3.4.3)\n","Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 2)) (1.22.4)\n","Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 2)) (3.20.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 2)) (2.27.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 2)) (67.7.2)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 2)) (0.7.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 2)) (1.8.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 2)) (2.3.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 2)) (0.40.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.0.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (4.39.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (23.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (9.5.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (2.8.2)\n","Requirement already satisfied: dash>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from open3d->-r requirements.txt (line 5)) (2.9.3)\n","Requirement already satisfied: nbformat==5.7.0 in /usr/local/lib/python3.10/dist-packages (from open3d->-r requirements.txt (line 5)) (5.7.0)\n","Requirement already satisfied: configargparse in /usr/local/lib/python3.10/dist-packages (from open3d->-r requirements.txt (line 5)) (1.5.3)\n","Requirement already satisfied: ipywidgets>=8.0.4 in /usr/local/lib/python3.10/dist-packages (from open3d->-r requirements.txt (line 5)) (8.0.6)\n","Requirement already satisfied: addict in /usr/local/lib/python3.10/dist-packages (from open3d->-r requirements.txt (line 5)) (2.4.0)\n","Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from open3d->-r requirements.txt (line 5)) (1.5.3)\n","Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from open3d->-r requirements.txt (line 5)) (6.0)\n","Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.10/dist-packages (from open3d->-r requirements.txt (line 5)) (1.2.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from open3d->-r requirements.txt (line 5)) (4.65.0)\n","Requirement already satisfied: pyquaternion in /usr/local/lib/python3.10/dist-packages (from open3d->-r requirements.txt (line 5)) (0.9.9)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat==5.7.0->open3d->-r requirements.txt (line 5)) (2.16.3)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat==5.7.0->open3d->-r requirements.txt (line 5)) (4.3.3)\n","Requirement already satisfied: jupyter-core in /usr/local/lib/python3.10/dist-packages (from nbformat==5.7.0->open3d->-r requirements.txt (line 5)) (5.3.0)\n","Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbformat==5.7.0->open3d->-r requirements.txt (line 5)) (5.7.1)\n","Requirement already satisfied: Flask>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d->-r requirements.txt (line 5)) (2.2.4)\n","Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d->-r requirements.txt (line 5)) (5.13.1)\n","Requirement already satisfied: dash-html-components==2.0.0 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d->-r requirements.txt (line 5)) (2.0.0)\n","Requirement already satisfied: dash-core-components==2.0.0 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d->-r requirements.txt (line 5)) (2.0.0)\n","Requirement already satisfied: dash-table==5.0.0 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d->-r requirements.txt (line 5)) (5.0.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 2)) (5.3.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 2)) (0.3.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 2)) (1.16.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 2)) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 2)) (1.3.1)\n","Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d->-r requirements.txt (line 5)) (5.5.6)\n","Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d->-r requirements.txt (line 5)) (7.34.0)\n","Requirement already satisfied: widgetsnbextension~=4.0.7 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d->-r requirements.txt (line 5)) (4.0.7)\n","Requirement already satisfied: jupyterlab-widgets~=3.0.7 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d->-r requirements.txt (line 5)) (3.0.7)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->open3d->-r requirements.txt (line 5)) (2022.7.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 2)) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 2)) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 2)) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 2)) (3.4)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d->-r requirements.txt (line 5)) (1.10.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d->-r requirements.txt (line 5)) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d->-r requirements.txt (line 5)) (3.1.0)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 2)) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8->-r requirements.txt (line 1)) (1.3.0)\n","Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.0.4->dash>=2.6.0->open3d->-r requirements.txt (line 5)) (2.1.2)\n","Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.0.4->dash>=2.6.0->open3d->-r requirements.txt (line 5)) (8.1.3)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets>=8.0.4->open3d->-r requirements.txt (line 5)) (0.2.0)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets>=8.0.4->open3d->-r requirements.txt (line 5)) (6.1.12)\n","Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets>=8.0.4->open3d->-r requirements.txt (line 5)) (6.3.1)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements.txt (line 5)) (0.18.2)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements.txt (line 5)) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements.txt (line 5)) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements.txt (line 5)) (3.0.38)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements.txt (line 5)) (2.14.0)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements.txt (line 5)) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements.txt (line 5)) (0.1.6)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements.txt (line 5)) (4.8.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat==5.7.0->open3d->-r requirements.txt (line 5)) (23.1.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat==5.7.0->open3d->-r requirements.txt (line 5)) (0.19.3)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.0.0->dash>=2.6.0->open3d->-r requirements.txt (line 5)) (8.2.2)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 2)) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 2)) (3.2.2)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core->nbformat==5.7.0->open3d->-r requirements.txt (line 5)) (3.3.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements.txt (line 5)) (0.8.3)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements.txt (line 5)) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements.txt (line 5)) (0.2.6)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=8.0.4->open3d->-r requirements.txt (line 5)) (23.2.1)\n"]}],"source":["!pip install -r requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"8nyddHkkpCVP"},"source":["# Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fGvXoJAmpD21","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684067630365,"user_tz":-120,"elapsed":9857,"user":{"displayName":"Alex Frasie","userId":"15148126022481281502"}},"outputId":"0bca156e-e08d-43db-f2ae-42f51947cfc1"},"outputs":[{"output_type":"stream","name":"stdout","text":["env: PYTORCH_CUDA_ALLOC_CONF=garbage_collection_threshold:0.6,max_split_size_mb:128\n"]}],"source":["import sys\n","import array\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import plotly.graph_objects as go\n","import json\n","import math\n","import gc\n","import os\n","\n","os.environ[\"OPENCV_IO_ENABLE_OPENEXR\"]=\"1\"\n","os.environ['DISPLAY']=\"0.0\"\n","import cv2\n","import open3d as o3d\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","import torchvision.models as models\n","from torch.utils.data import DataLoader, Dataset\n","from torch.utils.data import RandomSampler\n","from torch.utils.tensorboard import SummaryWriter\n","from tqdm import tqdm\n","\n","from torchmetrics.functional import peak_signal_noise_ratio, structural_similarity_index_measure, mean_squared_error\n","\n","import OpenEXR\n","import Imath\n","from kornia.contrib.extract_patches import extract_tensor_patches\n","%env PYTORCH_CUDA_ALLOC_CONF=garbage_collection_threshold:0.6,max_split_size_mb:128"]},{"cell_type":"markdown","metadata":{"id":"Ir4SSbmjpNx1"},"source":["# Constants"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"feyjzgq4pNKI"},"outputs":[],"source":["DATA_ROOT_DIR_NAME = \"./data\"\n","\n","TRAIN_DIR_NAME = DATA_ROOT_DIR_NAME + \"/train\"\n","VALID_DIR_NAME = DATA_ROOT_DIR_NAME + \"/valid\"\n","TEST_DIR_NAME = DATA_ROOT_DIR_NAME + \"/test\"\n","\n","CLEAN_DIR_NAME =  \"/clean\"\n","NOISY_DIR_NAME = \"/noisy\"\n","INFO_DIR_NAME = \"/info\"\n","\n","EXR_REGEX = \"/depth_S{:06d}_C{:02d}.exr\"\n","JSON_REGEX = \"/info_S{:06d}_C{:02d}.json\"\n","\n","INTRINSIC = \"intrinsic\"\n","EXTRINSIC = \"extrinsic\"\n","\n","CACHE_DIR_NAME = DATA_ROOT_DIR_NAME + \"/cache\"\n","TRAIN_CACHE_CLEAN = CACHE_DIR_NAME + \"/train_clean.pickle\"\n","VALID_CACHE_CLEAN = CACHE_DIR_NAME + \"/valid_clean.pickle\"\n","TEST_CACHE_CLEAN = CACHE_DIR_NAME + \"/test_clean.pickle\"\n","TRAIN_CACHE_NOISY = CACHE_DIR_NAME + \"/train_noisy.pickle\"\n","VALID_CACHE_NOISY = CACHE_DIR_NAME + \"/valid_noisy.pickle\"\n","TEST_CACHE_NOISY = CACHE_DIR_NAME + \"/test_noisy.pickle\"\n","\n","TENSORBOARD = SummaryWriter()\n","\n","SAVE_MODEL_NAME = 'unet_denoiser_model.pth'\n","\n","NO_CAMERAS = 3\n","NO_SAMPLES = 1500\n","TRAIN_SAMPLES = int(0.6 * NO_SAMPLES * NO_CAMERAS) // 3\n","VALID_SAMPLES = int(0.2 * NO_SAMPLES * NO_CAMERAS) // 3\n","TEST_SAMPLES = int(0.2 * NO_SAMPLES * NO_CAMERAS) // 3\n","\n","PATCH_SIZE = 64\n","STRIDE = PATCH_SIZE // 2\n","PADDING = 0\n","VALID_PATCH_THRESHOLD = (PATCH_SIZE * PATCH_SIZE) * 0.15\n","BATCH_SIZE = 128"]},{"cell_type":"markdown","metadata":{"id":"qXb60t1A978q"},"source":["# Data Gathering and Pre-Processing"]},{"cell_type":"markdown","metadata":{"id":"_TILcyr4A34d"},"source":["## Pre-process data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_5hPzhxMQVba"},"outputs":[],"source":["# B = Batch size\n","# N = number of patches\n","# C = channels\n","# H = height\n","# W = width\n","\n","def read_depth_image(file_path: str, scene: int, camera_no: int, clean:bool=True):\n","    # returns a tensor (C x H x W)\n","    depth_regex = file_path + CLEAN_DIR_NAME + EXR_REGEX if clean else file_path + NOISY_DIR_NAME + EXR_REGEX\n","    exr_file_name = depth_regex.format(scene, camera_no)\n","    depth = cv2.imread(exr_file_name, cv2.IMREAD_ANYCOLOR | cv2.IMREAD_ANYDEPTH)\n","    return T.ToTensor()(depth)\n","\n","def extract_samples(file_path: str, dataset: str, clean=True, start_range=None, end_range=None):\n","    samples = list()\n","    if start_range is None:\n","      start_range = 0 if dataset == 'train' else TRAIN_SAMPLES if dataset == 'valid' else TRAIN_SAMPLES + VALID_SAMPLES\n","    if end_range is None:\n","      end_range = TRAIN_SAMPLES if dataset == 'train' else TRAIN_SAMPLES + VALID_SAMPLES if dataset == 'valid' else NO_SAMPLES\n","    for scene in range(start_range, end_range):\n","        for camera in range(NO_CAMERAS):\n","            # Read each sample\n","            # (1024 x 1024) depth map size\n","            sample = read_depth_image(file_path, scene, camera, clean)\n","            # Add to final list\n","            samples.append(sample)\n","    return samples\n","\n","def extract_valid_patches(patches):\n","    # patches = tensor (N x H x W)\n","    no_patches = patches.shape[0]\n","    valid_patches = list()\n","    for idx in range(no_patches):\n","        patch = patches[idx,:]\n","        if torch.count_nonzero(patch) > VALID_PATCH_THRESHOLD:\n","            valid_patches.append(patch)\n","    return valid_patches\n","\n","def normalize_patch(patch):\n","    # patches = tensor (H x W)\n","    mask = patch > 0  # mask to only consider valid depth values\n","    valid_idx = torch.nonzero(patch)\n","    valid_values = patch[valid_idx[:,0], valid_idx[:,1]]\n","    std, mean = torch.std_mean(valid_values)\n","    eps = torch.tensor(1e-5)\n","    scale = torch.max(std, eps)\n","    patch = torch.where(patch > 0, (patch-mean)/scale, 0)\n","    return patch, scale\n","\n","def normalize_patches(patches):\n","    # patches = tensor (N x H x W)\n","    normalized_patches = list()\n","    for patch in patches:\n","        normalized_patch = normalize_patch(patch)\n","        normalized_patches.append(normalized_patch)\n","    return normalized_patches\n","\n","def extract_patches_from_sample(depth):\n","    # depth = tensor (C x H x W)\n","    # Split the depth map into patches\n","    depth = depth.unsqueeze(0) # add a dimention for B => (B x C x H x W)\n","    patches = extract_tensor_patches(depth, PATCH_SIZE, STRIDE, PADDING) # (B x N x C x H x W)\n","    patches = patches.squeeze(2) # remove channels\n","    patches = patches.squeeze(0) # remove batch size => (N x H x W)\n","    valid_patches = extract_valid_patches(patches)\n","    return valid_patches\n","\n","def extract_patches(samples):\n","    final_patches = list()\n","    for sample in tqdm(samples):\n","        patches = extract_patches_from_sample(sample)\n","        final_patches.extend(patches)\n","    return final_patches\n","\n","def compare_patches(patch, patch_nosisy):\n","    # Convert the depth map tensor to a NumPy array\n","    depth_map_np = patch.numpy()\n","    depth_map_np_n = patch_nosisy.numpy()\n","\n","    # Display the depth map using Matplotlib\n","    f, axarr = plt.subplots(1, 2, figsize=(10,10))\n","    axarr[0].imshow(depth_map_np, cmap='inferno')\n","    axarr[1].imshow(depth_map_np_n, cmap='inferno')\n","\n","def compare_patches_3(patch1, patch2, patch3):\n","    # Convert the depth map tensor to a NumPy array\n","    patch1_np = patch1.numpy()\n","    patch2_np = patch2.numpy()\n","    patch3_np = patch3.numpy()\n","\n","    # Display the depth map using Matplotlib\n","    f, axarr = plt.subplots(1, 3, figsize=(10,10))\n","    axarr[0].imshow(patch1_np, cmap='inferno')\n","    axarr[0].set_title('Clean')\n","    axarr[1].imshow(patch2_np, cmap='inferno')\n","    axarr[1].set_title('Predicted')\n","    axarr[2].imshow(patch3_np, cmap='inferno')\n","    axarr[2].set_title('Noisy')"]},{"cell_type":"markdown","metadata":{"id":"cpO5iSWQVdxj"},"source":["## Define dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vmRe_uPfVgYq"},"outputs":[],"source":["class DepthMapDataset(Dataset):\n","    def __init__(self, file_path, dataset, start, end):\n","        self.clean_data = self.extract_data(file_path, dataset, True, start, end)\n","        self.noisy_data = self.extract_data(file_path, dataset, False, start, end)\n","    \n","    def extract_data(self, file_path, dataset, clean:bool, start=None, end=None):\n","        data = self.get_cached_files(dataset, clean)\n","        clean_noisy = 'clean' if clean else 'noisy'\n","        if data is not None:\n","            print(f\"Found cached files for {clean_noisy} {dataset} dataset with {len(data)} elements.\")\n","            return data\n","        samples = extract_samples(file_path, dataset, clean, start, end)\n","        patches = extract_patches(samples)\n","        self.cache_files(dataset, patches, clean)\n","        print(f\"Cached files for {clean_noisy} {dataset} dataset.\")\n","        return patches\n","\n","    def cache_files(self, dataset, data, clean):\n","        if dataset == 'train':\n","            if clean:\n","                torch.save(data, TRAIN_CACHE_CLEAN)\n","            else:\n","                torch.save(data, TRAIN_CACHE_NOISY)\n","        elif dataset == 'valid':\n","            if clean:\n","                torch.save(data, VALID_CACHE_CLEAN)\n","            else:\n","                torch.save(data, VALID_CACHE_NOISY)\n","        elif dataset == 'test':\n","            if clean:\n","                torch.save(data, TEST_CACHE_CLEAN)\n","            else:\n","                torch.save(data, TEST_CACHE_NOISY)\n","\n","    def get_cached_files(self, dataset, clean):\n","        data = None\n","        if dataset == 'train' and clean:\n","            data = torch.load(TRAIN_CACHE_CLEAN) if os.path.exists(TRAIN_CACHE_CLEAN) else None\n","        elif dataset == 'train' and not clean:\n","            data = torch.load(TRAIN_CACHE_NOISY) if os.path.exists(TRAIN_CACHE_NOISY) else None\n","        elif dataset == 'valid' and clean:\n","            data = torch.load(VALID_CACHE_CLEAN) if os.path.exists(VALID_CACHE_CLEAN) else None\n","        elif dataset == 'valid' and not clean:\n","            data = torch.load(VALID_CACHE_NOISY) if os.path.exists(VALID_CACHE_NOISY) else None\n","        elif dataset == 'test' and clean:\n","            data = torch.load(TEST_CACHE_CLEAN) if os.path.exists(TEST_CACHE_CLEAN) else None\n","        elif dataset == 'test' and not clean:\n","            data = torch.load(TEST_CACHE_NOISY) if os.path.exists(TEST_CACHE_NOISY) else None\n","        return data\n","        \n","    def __len__(self):\n","        return len(self.clean_data)\n","    \n","    def __getitem__(self, index):\n","        clean_patch = self.clean_data[index]\n","        noisy_patch = self.noisy_data[index]\n","        return clean_patch, noisy_patch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U8CGJUykDUoA"},"outputs":[],"source":["def get_loader(file_path=TRAIN_DIR_NAME, dataset='train', start=0, end=100, batch_size=32, num_workers=4, shuffle=False, pin_memory=True):\n","    dataset = DepthMapDataset(file_path, dataset, start, end)\n","    loader = DataLoader(\n","        dataset=dataset, \n","        batch_size=batch_size,\n","        num_workers=num_workers,\n","        shuffle=shuffle, \n","        pin_memory=pin_memory)\n","    \n","    return loader, dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A7XN4JeV4_4T","outputId":"2df22ac0-3156-40a4-df2f-5f973011a5b3","executionInfo":{"status":"ok","timestamp":1684067773480,"user_tz":-120,"elapsed":95428,"user":{"displayName":"Alex Frasie","userId":"15148126022481281502"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Found cached files for clean train dataset with 76948 elements.\n","Found cached files for noisy train dataset with 76948 elements.\n","Found cached files for clean valid dataset with 19543 elements.\n","Found cached files for noisy valid dataset with 19543 elements.\n","Found cached files for clean test dataset with 4034 elements.\n","Found cached files for noisy test dataset with 4034 elements.\n"]}],"source":["train_loader, train_dataset = get_loader(file_path=TRAIN_DIR_NAME,\n","                                         dataset='train',\n","                                         start=0,\n","                                         end=120,\n","                                         batch_size=BATCH_SIZE, \n","                                         shuffle=False)\n","valid_loader, valid_dataset = get_loader(file_path=VALID_DIR_NAME,\n","                                         dataset='valid',\n","                                         start=900,\n","                                         end=940,\n","                                         batch_size=BATCH_SIZE, \n","                                         shuffle=False)\n","test_loader, test_dataset = get_loader(file_path=TEST_DIR_NAME,\n","                                       dataset='test',\n","                                       start=1200,\n","                                       end=1210,\n","                                       batch_size=BATCH_SIZE, \n","                                       shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"OcNWCh8RVn7B"},"source":["# Model"]},{"cell_type":"markdown","metadata":{"id":"A3DEsV4AVSmh"},"source":["## Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"48NZKEbyVR0j"},"outputs":[],"source":["LEARNING_RATE = 8e-4\n","EPOCHS = 10\n","PRETRAIN_EPOCHS = 30\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"nh5RqY7nWLxZ"},"source":["## Model Architecture"]},{"cell_type":"markdown","source":["### Greedy Layer-Wise Unsupervised Pretraining "],"metadata":{"id":"8Y1QwjmExBC8"}},{"cell_type":"code","source":["class InputLayerPretrainingAE(nn.Module):\n","    def __init__(self, ch_in, ch_out):\n","        super(InputLayerPretrainingAE, self).__init__()\n","        self.conv = nn.Conv2d(ch_in, ch_out, kernel_size=3, padding=1)  \n","        self.conv_t = nn.ConvTranspose2d(ch_out, ch_in, kernel_size=3, padding=1)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv(x))\n","        x = F.relu(self.conv_t(x))         \n","        return x"],"metadata":{"id":"xE2K9VVJHJ_i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class InnnerLayer1PretrainingAE(nn.Module):\n","    def __init__(self, ch_in, ch_out):\n","        super(InnnerLayer1PretrainingAE, self).__init__()\n","        ch_inner = ch_in // 2\n","        self.conv_1 = nn.Conv2d(ch_in, ch_inner, kernel_size=3, padding=1)\n","        self.conv_2 = nn.Conv2d(ch_in + ch_inner, ch_out, kernel_size=3, padding=1)\n"," \n","        self.conv_t_1 = nn.ConvTranspose2d(ch_out, ch_in + ch_inner, kernel_size=4, padding=1)\n","        self.conv_t_2 = nn.ConvTranspose2d(ch_in + ch_inner, ch_in, kernel_size=4, padding=1)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv_1(x))\n","        x = F.relu(self.conv_2(x))\n","        x = F.relu(self.conv_t_1(x))\n","        x = F.relu(self.conv_t_2(x))\n","        return x"],"metadata":{"id":"0A1DpB9IHodx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Pretraining Input Layer"],"metadata":{"id":"rH85l2hbJS1Z"}},{"cell_type":"code","source":["gc.collect()\n","torch.cuda.empty_cache()\n","torch.autograd.set_detect_anomaly(True)\n","\n","# Define the denoising autoencoder model and optimizer\n","model = InputLayerPretrainingAE(1, 16).to(device)\n","print(f\"Running on device {device} for InputLayerPretrainingAE\")\n","optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","# Define the loss function\n","criterion = nn.MSELoss()\n","\n","# Train the denoising autoencoder\n","for epoch in range(PRETRAIN_EPOCHS):\n","    # Training\n","    train_loss = 0.0\n","    model.train()\n","    for idx, data in enumerate(tqdm(train_loader)):\n","        clean_patch, noisy_patch = data\n","\n","        # Send data to cuda\n","        if torch.cuda.is_available():\n","            noisy_patch = noisy_patch.cuda()\n","\n","        # Normalize pathces\n","        normalized_noisy_patch, scale = normalize_patch(noisy_patch)\n","        normalized_noisy_patch = normalized_noisy_patch.unsqueeze(1)\n","\n","        # Zero the gradients\n","        optimizer.zero_grad()\n","        \n","        # Forward pass\n","        output = model(normalized_noisy_patch)\n","\n","        # De-normalize\n","        output = output.squeeze(1)\n","        output = torch.mul(output, scale)\n","        \n","        # Compute the loss\n","        loss = criterion(output, noisy_patch)\n","\n","        # Backward pass and optimize\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss = train_loss + loss.item()\n","    TENSORBOARD.add_scalar(\"Pre-training InputLayerPretrainingAE loss\", train_loss, epoch)\n","    print(f\"Epoch [{epoch+1}/{EPOCHS}]: Train Loss: {train_loss:.6f}\")\n","TENSORBOARD.flush()\n","TENSORBOARD.close()"],"metadata":{"id":"M99YOYya2GUJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684068035225,"user_tz":-120,"elapsed":153933,"user":{"displayName":"Alex Frasie","userId":"15148126022481281502"}},"outputId":"ad00b438-13a8-416d-86c5-2ab01a4f5db0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running on device cuda:0 for InputLayerPretrainingAE\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 602/602 [00:05<00:00, 102.97it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/10]: Train Loss: 158.772846\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 602/602 [00:05<00:00, 120.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [2/10]: Train Loss: 88.060491\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 602/602 [00:05<00:00, 116.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [3/10]: Train Loss: 85.419655\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 602/602 [00:05<00:00, 118.40it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [4/10]: Train Loss: 83.646832\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 602/602 [00:05<00:00, 116.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [5/10]: Train Loss: 82.206766\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 602/602 [00:04<00:00, 121.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [6/10]: Train Loss: 80.974592\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 602/602 [00:04<00:00, 122.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [7/10]: Train Loss: 79.959357\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 602/602 [00:05<00:00, 118.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [8/10]: Train Loss: 79.098727\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 602/602 [00:05<00:00, 117.70it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [9/10]: Train Loss: 78.300343\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 602/602 [00:05<00:00, 120.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [10/10]: Train Loss: 77.478631\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 602/602 [00:05<00:00, 117.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [11/10]: Train Loss: 76.718261\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 602/602 [00:05<00:00, 120.27it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [12/10]: Train Loss: 76.152082\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 602/602 [00:05<00:00, 119.02it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [13/10]: Train Loss: 75.633844\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 602/602 [00:05<00:00, 118.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [14/10]: Train Loss: 75.148022\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 602/602 [00:04<00:00, 122.91it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [15/10]: Train Loss: 74.706849\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 602/602 [00:05<00:00, 117.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [16/10]: Train Loss: 74.293692\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 602/602 [00:05<00:00, 120.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [17/10]: Train Loss: 73.907569\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 602/602 [00:04<00:00, 121.01it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [18/10]: Train Loss: 73.544148\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 602/602 [00:05<00:00, 115.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [19/10]: Train Loss: 73.208178\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 602/602 [00:05<00:00, 119.43it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [20/10]: Train Loss: 72.886385\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 602/602 [00:05<00:00, 119.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [21/10]: Train Loss: 72.582083\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 602/602 [00:05<00:00, 119.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [22/10]: Train Loss: 72.293182\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 602/602 [00:04<00:00, 121.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [23/10]: Train Loss: 72.018085\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 602/602 [00:05<00:00, 115.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [24/10]: Train Loss: 71.756047\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 602/602 [00:05<00:00, 116.07it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [25/10]: Train Loss: 71.504920\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 602/602 [00:05<00:00, 115.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [26/10]: Train Loss: 71.264614\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 602/602 [00:05<00:00, 110.94it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [27/10]: Train Loss: 71.033795\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 602/602 [00:05<00:00, 113.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [28/10]: Train Loss: 70.812190\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 602/602 [00:05<00:00, 113.35it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [29/10]: Train Loss: 70.598999\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 602/602 [00:05<00:00, 115.21it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [30/10]: Train Loss: 70.393576\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["## Pretrain First Inner Layer"],"metadata":{"id":"lHvuIK8XS2F5"}},{"cell_type":"code","source":["class PretrainedInputLayerDataset(Dataset):\n","    def __init__(self, dataset: DepthMapDataset):\n","        self.clean_data = self.transform_data(dataset.clean_data)\n","        self.noisy_data = self.transform_data(dataset.noisy_data)\n","    \n","    def transform_data(self, data):\n","        new_data = list()\n","        for idx in range(len(data) // 2):\n","            patch = data[idx] # 64 x 64\n","            patch_processed = patch.unsqueeze(0).unsqueeze(0) # 1 x 1 x 64 x 64\n","            patch_after_input_layer = model(patch_processed)\n","            new_data.append(patch_after_input_layer)\n","        return new_data\n","      \n","    def __len__(self):\n","        return len(self.clean_data)\n","    \n","    def __getitem__(self, index):\n","        clean_patch = self.clean_data[index]\n","        noisy_patch = self.noisy_data[index]\n","        return clean_patch, noisy_patch"],"metadata":{"id":"beFCLmRhTa8z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_loader_for_inner_1(init_dataset, batch_size=32, num_workers=4, shuffle=False, pin_memory=True):\n","    dataset = PretrainedInputLayerDataset(init_dataset)\n","    loader = DataLoader(\n","        dataset=dataset, \n","        batch_size=batch_size,\n","        num_workers=num_workers,\n","        shuffle=shuffle, \n","        pin_memory=pin_memory)\n","    \n","    return loader, dataset"],"metadata":{"id":"vN5_f6YcVZtr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model.to('cpu')\n","# train_loader_inner1, train_dataset_inner1 = get_loader_for_inner_1(init_dataset=train_dataset,\n","#                                                                    batch_size=BATCH_SIZE, \n","#                                                                    shuffle=True)\n","# valid_loader_inner1, valid_dataset_inner1 = get_loader_for_inner_1(init_dataset=valid_dataset,\n","#                                                                    batch_size=BATCH_SIZE,\n","#                                                                    shuffle=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":432},"id":"qJ3veDBZVkjM","executionInfo":{"status":"error","timestamp":1684070591303,"user_tz":-120,"elapsed":129141,"user":{"displayName":"Alex Frasie","userId":"15148126022481281502"}},"outputId":"88cc6e87-d5fe-4275-ea6e-d778f11c552e"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/lib/python3.10/linecache.py\u001b[0m in \u001b[0;36mcheckcache\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mstat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfullname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '<ipython-input-17-1114f7d80daf>'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-e91edd830aaa>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m train_loader_inner1, train_dataset_inner1 = get_loader_for_inner_1(init_dataset=train_dataset,\n\u001b[0m\u001b[1;32m      3\u001b[0m                                                                    \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                                    shuffle=True)\n\u001b[1;32m      5\u001b[0m valid_loader_inner1, valid_dataset_inner1 = get_loader_for_inner_1(init_dataset=valid_dataset,\n","\u001b[0;32m<ipython-input-22-194e45bd53ed>\u001b[0m in \u001b[0;36mget_loader_for_inner_1\u001b[0;34m(init_dataset, batch_size, num_workers, shuffle, pin_memory)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_loader_for_inner_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedInputLayerDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     loader = DataLoader(\n\u001b[1;32m      4\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-26-7b10fd7c6292>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mPretrainedInputLayerDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDepthMapDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoisy_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoisy_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-26-7b10fd7c6292>\u001b[0m in \u001b[0;36mtransform_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mpatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# 64 x 64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mpatch_processed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 1 x 1 x 64 x 64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mpatch_after_input_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch_processed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch_after_input_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-1114f7d80daf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_t\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1455\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1458\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# fallback to traceback.format_stack()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/traceback.py\u001b[0m in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m     \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStackSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwalk_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m     \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/traceback.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    377\u001b[0m                 filename, lineno, name, lookup_line=False, locals=f_locals))\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfnames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m         \u001b[0;31m# If immediate lookup was desired, trigger lookups now.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlookup_lines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/compilerop.py\u001b[0m in \u001b[0;36mcheck_linecache_ipython\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \"\"\"\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# First call the original checkcache as intended\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkcache_ori\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0;31m# Then, update back the cache with our data, so that tracebacks related\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;31m# to our compiled codes can be produced.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/linecache.py\u001b[0m in \u001b[0;36mcheckcache\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mcontinue\u001b[0m   \u001b[0;31m# no-op for files loaded via a __loader__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mstat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfullname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["print(len(train_dataset_inner1))\n","print(len(valid_dataset_inner1))\n","print(len(test_dataset_inner1))"],"metadata":{"id":"jebMnVBkVtSp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Save pretrained weights"],"metadata":{"id":"8Ocp2rcj2BOn"}},{"cell_type":"code","source":["for param_tensor in model.state_dict():\n","    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6tofyqz7RpHb","executionInfo":{"status":"ok","timestamp":1684068039028,"user_tz":-120,"elapsed":4,"user":{"displayName":"Alex Frasie","userId":"15148126022481281502"}},"outputId":"fd309629-e9f9-4935-86bd-8e74ea806de3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["conv.weight \t torch.Size([16, 1, 3, 3])\n","conv.bias \t torch.Size([16])\n","conv_t.weight \t torch.Size([16, 1, 3, 3])\n","conv_t.bias \t torch.Size([1])\n"]}]},{"cell_type":"code","source":["weights = {\n","    'conv_input.weight': model.state_dict()['conv.weight'],\n","    'conv_input.bias': model.state_dict()['conv.bias']\n","}\n","torch.save(weights, 'pretrained_weights_input_layer.pth')"],"metadata":{"id":"ILcgkJKzSVdi"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyMH4bZ50iz5TIyfKAr38h8z"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}